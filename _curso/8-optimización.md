---
title: "Optimización"
---

Desafortunadamente, nuestros ordenadores tienen una capacidad de cómputo limitada.
Por lo tanto es necesario necesario usar algunos truquillos para aligerar la cosa.
En este capítulo veremos como podemos hacer el proceso de **entrenamiento más rápido**.


* Stochastic Gradient Descent
* Momentum
* Nesterov Momentum
* Inicialización de parámetros
* Cambiar el lerning rate
  * AdaGrad
  * RMSProp
  * [Adam](https://arxiv.org/abs/1412.6980)
  * Choosing the Right Optimization Algorithm
* Approximación de segundo orden
  * Método de Newtown
  * Conjugate Gradients
  * BFGS
* Otras estrategias
  * Batch Normalization
  * Coordinate Descent
  * Polyak Averaging
  * Supervised Pretraining


* ReLU

## Referencias

* [un blog](https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/)
* [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
